{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cbe389",
   "metadata": {},
   "source": [
    "# Tech Challenge - Fase 3: Previsão de Ações GOOG com Machine Learning\n",
    "\n",
    "**Aluno:** Mateus Bressan\n",
    "**Curso:** Pós-Graduação em Machine Learning Engineering\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Bem-vindos à apresentação do Tech Challenge da Fase 3! O objetivo deste projeto é desenvolver uma solução completa de Machine Learning para prever o preço de fechamento das ações da Google (GOOG). \n",
    "\n",
    "O desafio envolveu as seguintes etapas principais, conforme solicitado: \n",
    "1.  **Coleta e Armazenamento de Dados:** Obter dados históricos e \"em tempo real\" das ações e armazená-los em um banco de dados.\n",
    "2.  **Modelagem de Machine Learning:** Treinar um modelo para prever o preço das ações. \n",
    "3.  **API e Dashboard:** Criar uma API para servir as previsões e um dashboard para visualização. \n",
    "4.  **Documentação:** Manter o código documentado e versionado no GitHub.\n",
    "\n",
    "Vamos detalhar cada uma dessas etapas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e84b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas instaladas!\n",
      "Bibliotecas importadas!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Instalação das principais dependências\n",
    "!pip install yfinance pandas numpy scikit-learn matplotlib sqlalchemy psycopg2-binary python-dotenv flask joblib --quiet\n",
    "\n",
    "print(\"Bibliotecas instaladas!\")\n",
    "\n",
    "# Importações principais utilizadas no projeto\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sqlalchemy import create_engine, Column, Integer, Float, Date, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "\n",
    "\n",
    "print(\"Bibliotecas importadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b4ccf",
   "metadata": {},
   "source": [
    "## 2. Coleta de Dados\n",
    "\n",
    "A primeira etapa crucial é a coleta de dados. Utilizamos a biblioteca `yfinance` para buscar dados históricos e do dia (simulando tempo real) das ações da GOOG. [cite: main.py]\n",
    "\n",
    "Criamos uma classe `ColetorDeDados` para organizar essa funcionalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e245e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coletando dados históricos para GOOG de 2022-01-01 até 2025-04-03\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados históricos coletados: 815 registros.\n",
      "\n",
      "Primeiros 5 registros históricos:\n",
      "Price  data_pregao    abertura        alta       baixa  fechamento    volume\n",
      "Ticker                    GOOG        GOOG        GOOG        GOOG      GOOG\n",
      "0       2022-01-03  143.794404  144.863847  142.825999  144.390579  25214000\n",
      "1       2022-01-04  144.864359  145.918859  143.138176  143.735703  22928000\n",
      "2       2022-01-05  143.501302  143.617755  136.875186  137.004578  49642000\n",
      "3       2022-01-06  136.849312  139.027502  136.118779  136.902557  29050000\n",
      "4       2022-01-07  137.254895  137.602993  135.148873  136.358643  19408000\n",
      "Coletando dados 'em tempo real' (intraday) para GOOG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados 'em tempo real' coletados: 390 registros.\n",
      "\n",
      "Últimos 5 registros 'em tempo real' (se disponíveis):\n",
      "Price                       data    abertura        alta       baixa  \\\n",
      "Ticker                                  GOOG        GOOG        GOOG   \n",
      "385    2025-04-04 19:55:00+00:00  148.550003  148.570007  147.699997   \n",
      "386    2025-04-04 19:56:00+00:00  147.880005  148.160004  147.740005   \n",
      "387    2025-04-04 19:57:00+00:00  147.860001  147.860001  147.550003   \n",
      "388    2025-04-04 19:58:00+00:00  147.649994  147.839996  147.649994   \n",
      "389    2025-04-04 19:59:00+00:00  147.729996  147.839996  147.639999   \n",
      "\n",
      "Price   fechamento  volume  \n",
      "Ticker        GOOG    GOOG  \n",
      "385     147.889999  543889  \n",
      "386     147.850006  288737  \n",
      "387     147.639999  283057  \n",
      "388     147.729996  378320  \n",
      "389     147.639999  898314  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Definição da Classe ColetorDeDados (igual ao Colab)\n",
    "class ColetorDeDados:\n",
    "    \"\"\"Classe para coleta de dados históricos e 'em tempo real' de ações.\"\"\"\n",
    "\n",
    "    def __init__(self, ticker, data_inicio=None, data_fim=None):\n",
    "        self.ticker = ticker\n",
    "        self.data_inicio = data_inicio\n",
    "        self.data_fim = data_fim if data_fim else datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    def coletar_dados_historicos(self):\n",
    "        \"\"\"Coleta dados históricos de ações.\"\"\"\n",
    "        print(f\"Coletando dados históricos para {self.ticker} de {self.data_inicio} até {self.data_fim}\")\n",
    "        dados = yf.download(self.ticker, start=self.data_inicio, end=self.data_fim)\n",
    "        if dados.empty:\n",
    "            print(f\"Nenhum dado histórico encontrado para {self.ticker} no período.\")\n",
    "            return pd.DataFrame()\n",
    "        dados.reset_index(inplace=True)\n",
    "        # Renomeando colunas para o padrão do projeto\n",
    "        dados.rename(columns={'Date': 'data_pregao', 'Open': 'abertura', 'High': 'alta', 'Low': 'baixa', 'Close': 'fechamento', 'Volume': 'volume'}, inplace=True)\n",
    "        # Selecionando e ordenando colunas relevantes\n",
    "        dados = dados[['data_pregao', 'abertura', 'alta', 'baixa', 'fechamento', 'volume']]\n",
    "        # Converter a coluna de data para apenas data (sem hora) se necessário\n",
    "        dados['data_pregao'] = pd.to_datetime(dados['data_pregao']).dt.date\n",
    "        print(f\"Dados históricos coletados: {dados.shape[0]} registros.\")\n",
    "        return dados\n",
    "\n",
    "    def coletar_dados_tempo_real(self):\n",
    "        \"\"\"Coleta dados intraday (intervalo de 1 minuto) do dia atual.\"\"\"\n",
    "        print(f\"Coletando dados 'em tempo real' (intraday) para {self.ticker}\")\n",
    "        # Usamos period='1d' e interval='1m' para pegar os dados mais recentes do dia\n",
    "        dados = yf.download(self.ticker, period='1d', interval='1m')\n",
    "        if not dados.empty:\n",
    "            # Renomeando colunas\n",
    "            dados.rename(columns={'Open': 'abertura', 'High': 'alta', 'Low': 'baixa', 'Close': 'fechamento', 'Volume': 'volume'}, inplace=True)\n",
    "            dados = dados[['abertura', 'alta', 'baixa', 'fechamento', 'volume']]\n",
    "            dados.reset_index(inplace=True)\n",
    "            dados.rename(columns={'Datetime': 'data'}, inplace=True)\n",
    "             # Converter a coluna de data para datetime\n",
    "            dados['data'] = pd.to_datetime(dados['data'])\n",
    "            print(f\"Dados 'em tempo real' coletados: {dados.shape[0]} registros.\")\n",
    "        else:\n",
    "            print(\"Não foram encontrados dados 'em tempo real' no momento (mercado pode estar fechado).\")\n",
    "        return dados\n",
    "\n",
    "# Exemplo de uso da coleta de dados históricos\n",
    "ticker = \"GOOG\"\n",
    "data_inicio_historico = \"2023-01-01\" # Ajuste a data inicial se desejar\n",
    "data_fim_historico = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\") # Até ontem\n",
    "\n",
    "coletor_hist = ColetorDeDados(ticker, data_inicio_historico, data_fim_historico)\n",
    "dados_historicos = coletor_hist.coletar_dados_historicos()\n",
    "\n",
    "if not dados_historicos.empty:\n",
    "    print(\"\\nPrimeiros 5 registros históricos:\")\n",
    "    print(dados_historicos.head())\n",
    "else:\n",
    "    print(\"\\nNão foi possível coletar dados históricos.\")\n",
    "\n",
    "# Exemplo de uso da coleta de dados \"em tempo real\"\n",
    "coletor_rt = ColetorDeDados(ticker)\n",
    "dados_rt = coletor_rt.coletar_dados_tempo_real()\n",
    "\n",
    "if not dados_rt.empty:\n",
    "    print(\"\\nÚltimos 5 registros 'em tempo real' (se disponíveis):\")\n",
    "    print(dados_rt.tail())\n",
    "else:\n",
    "    print(\"\\nNão há dados 'em tempo real' disponíveis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d27b5",
   "metadata": {},
   "source": [
    "## 3. Armazenamento de Dados\n",
    "\n",
    "Os dados coletados precisam ser armazenados de forma persistente. Para isso, escolhemos um banco de dados PostgreSQL e utilizamos SQLAlchemy como ORM (Object-Relational Mapper) para facilitar a interação entre o Python e o banco. [cite: main.py, 6]\n",
    "\n",
    "Definimos um modelo (`HistoricoAcao`) que representa a tabela no banco e funções para inserir e consultar os dados.\n",
    "\n",
    "*Observação: A execução das funções de banco de dados abaixo requer um servidor PostgreSQL configurado e acessível, com as credenciais definidas em um arquivo `.env` ou variáveis de ambiente.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63fb7df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Código do Banco de Dados (SQLAlchemy) ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Este código TENTA se conectar ao banco de dados PostgreSQL local.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Certifique-se de que o servidor está rodando e o .env está configurado.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Configuração do SQLAlchemy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m DATABASE_URL = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpostgresql://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mos\u001b[49m.getenv(\u001b[33m'\u001b[39m\u001b[33mDB_USER\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.getenv(\u001b[33m'\u001b[39m\u001b[33mDB_PASSWORD\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.getenv(\u001b[33m'\u001b[39m\u001b[33mDB_HOST\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.getenv(\u001b[33m'\u001b[39m\u001b[33mDB_NAME\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m engine = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      8\u001b[39m Session = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Código do Banco de Dados (SQLAlchemy) ---\n",
    "# Este código TENTA se conectar ao banco de dados PostgreSQL local.\n",
    "# Certifique-se de que o servidor está rodando e o .env está configurado.\n",
    "\n",
    "# Configuração do SQLAlchemy\n",
    "DATABASE_URL = f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_NAME')}\"\n",
    "engine = None\n",
    "Session = None\n",
    "Base = declarative_base()\n",
    "db_connection_ok = False\n",
    "\n",
    "try:\n",
    "    print(f\"Tentando conectar ao banco: {DATABASE_URL.replace(os.getenv('DB_PASSWORD', '****'), '****')}\") # Esconde a senha no print\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    # Testa a conexão\n",
    "    with engine.connect() as connection:\n",
    "         print(\"Conexão com o banco de dados estabelecida com sucesso!\")\n",
    "         db_connection_ok = True\n",
    "    Session = sessionmaker(bind=engine)\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Não foi possível conectar ao banco de dados.\")\n",
    "    print(f\"Detalhe do erro: {e}\")\n",
    "    print(\"Verifique se o servidor PostgreSQL está rodando e se as credenciais no arquivo .env estão corretas.\")\n",
    "    print(\"As operações de banco de dados serão puladas.\")\n",
    "\n",
    "# Modelo da tabela (igual ao original)\n",
    "class HistoricoAcao(Base):\n",
    "    __tablename__ = 'historico_acao'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    data_pregao = Column(Date, nullable=False)\n",
    "    abertura = Column(Float)\n",
    "    alta = Column(Float)\n",
    "    baixa = Column(Float)\n",
    "    fechamento = Column(Float)\n",
    "    volume = Column(Integer)\n",
    "\n",
    "# Função para criar a tabela se não existir\n",
    "def criar_tabela_se_nao_existir():\n",
    "    if engine:\n",
    "        try:\n",
    "            print(f\"Verificando/Criando tabela '{HistoricoAcao.__tablename__}'...\")\n",
    "            Base.metadata.create_all(engine)\n",
    "            print(\"Tabela verificada/criada.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao verificar/criar tabela: {e}\")\n",
    "\n",
    "# Função para armazenar dados (usando a conexão real)\n",
    "def armazenar_dados_rds(dados, session_factory):\n",
    "    if not session_factory or dados.empty:\n",
    "        print(\"Armazenamento pulado (sem sessão de DB ou sem dados).\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Armazenando dados na tabela '{HistoricoAcao.__tablename__}' ---\")\n",
    "    session = session_factory()\n",
    "    try:\n",
    "        # Exclui os dados antigos (conforme main.py)\n",
    "        print(\"Excluindo dados antigos...\")\n",
    "        num_deleted = session.query(HistoricoAcao).delete()\n",
    "        session.commit()\n",
    "        print(f\"{num_deleted} registros antigos excluídos.\")\n",
    "\n",
    "        # Insere os novos dados\n",
    "        print(f\"Inserindo {len(dados)} novos registros...\")\n",
    "        registros_obj = []\n",
    "        for _, row in dados.iterrows():\n",
    "             # Certifica que a data está no formato date, não datetime\n",
    "            data_corrigida = row['data_pregao']\n",
    "            if isinstance(data_corrigida, datetime):\n",
    "                data_corrigida = data_corrigida.date()\n",
    "\n",
    "            registro = HistoricoAcao(\n",
    "                data_pregao=data_corrigida,\n",
    "                abertura=row['abertura'],\n",
    "                alta=row['alta'],\n",
    "                baixa=row['baixa'],\n",
    "                fechamento=row['fechamento'],\n",
    "                volume=int(row['volume']) if pd.notna(row['volume']) else None # Trata volume NaN\n",
    "            )\n",
    "            registros_obj.append(registro)\n",
    "\n",
    "        session.add_all(registros_obj)\n",
    "        session.commit()\n",
    "        print(\"Dados armazenados com sucesso no banco de dados.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao armazenar dados no RDS: {e}\")\n",
    "        session.rollback()\n",
    "    finally:\n",
    "        session.close()\n",
    "    print(\"--- Fim do armazenamento ---\")\n",
    "\n",
    "\n",
    "# Função para ler dados (usando a conexão real)\n",
    "def ler_dados_rds(session_factory):\n",
    "    if not session_factory:\n",
    "        print(\"Leitura pulada (sem sessão de DB). Retornando DataFrame vazio.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"\\n--- Lendo dados da tabela '{HistoricoAcao.__tablename__}' ---\")\n",
    "    session = session_factory()\n",
    "    try:\n",
    "        query = session.query(HistoricoAcao).order_by(HistoricoAcao.data_pregao)\n",
    "        dados_lidos = pd.read_sql(query.statement, engine)\n",
    "        print(f\"{len(dados_lidos)} registros lidos do banco de dados.\")\n",
    "        return dados_lidos\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler dados do RDS: {e}\")\n",
    "        return pd.DataFrame() # Retorna DataFrame vazio em caso de erro\n",
    "    finally:\n",
    "        session.close()\n",
    "    print(\"--- Fim da leitura ---\")\n",
    "\n",
    "# Fluxo Principal do Banco de Dados (só executa se a conexão foi ok)\n",
    "dados_para_analise = pd.DataFrame() # Inicializa vazio\n",
    "\n",
    "if db_connection_ok:\n",
    "    criar_tabela_se_nao_existir()\n",
    "    if not dados_historicos.empty:\n",
    "        armazenar_dados_rds(dados_historicos, Session)\n",
    "        dados_para_analise = ler_dados_rds(Session)\n",
    "    else:\n",
    "         print(\"Não há dados históricos para armazenar ou ler do banco.\")\n",
    "         # Tenta ler o que já existe no banco caso a coleta falhe mas o banco funcione\n",
    "         print(\"Tentando ler dados existentes no banco...\")\n",
    "         dados_para_analise = ler_dados_rds(Session)\n",
    "\n",
    "else:\n",
    "    print(\"\\nUsando dados históricos em memória (coletados anteriormente) pois não há conexão com o DB.\")\n",
    "    # Usa os dados coletados se a conexão com o DB falhar\n",
    "    dados_para_analise = dados_historicos.copy()\n",
    "\n",
    "\n",
    "if not dados_para_analise.empty:\n",
    "    print(\"\\nDados carregados para análise (do Banco de Dados ou Memória):\")\n",
    "    print(dados_para_analise.head())\n",
    "    print(f\"\\nTotal de registros para análise: {len(dados_para_analise)}\")\n",
    "    # Garante que a coluna de data seja datetime para plotagem\n",
    "    dados_para_analise['data_pregao'] = pd.to_datetime(dados_para_analise['data_pregao'])\n",
    "else:\n",
    "    print(\"\\nERRO: Não foi possível obter dados para análise (nem do DB, nem da coleta). Verifique os passos anteriores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c26c0d",
   "metadata": {},
   "source": [
    "## 4. Exploração e Pré-processamento de Dados\n",
    "\n",
    "Antes de treinar o modelo, é essencial explorar e pré-processar os dados lidos do banco (ou da nossa simulação aqui).\n",
    "\n",
    "**Etapas:**\n",
    "1.  **Análise Exploratória:** Visualizar a série temporal do preço de fechamento. [cite: main.py]\n",
    "2.  **Engenharia de Features:** Criar novas features que podem ajudar o modelo, como:\n",
    "    * Média Móvel (7 dias): Suaviza a série e indica tendência.\n",
    "    * Retorno Diário (%): Variação percentual do preço de fechamento. [cite: main.py]\n",
    "3.  **Limpeza:** Remover valores ausentes (`NaN`) que surgem após o cálculo da média móvel e retorno diário. [cite: main.py]\n",
    "4.  **Normalização:** Colocar as features numéricas na mesma escala (0 a 1) usando `MinMaxScaler`. Isso é importante para muitos algoritmos de ML. [cite: main.py]\n",
    "5.  **Divisão Treino/Teste:** Separar os dados em conjuntos de treino e teste para avaliar o modelo de forma justa. Usamos `shuffle=False` para manter a ordem temporal. [cite: main.py]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3217aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando os dados carregados na célula anterior (do DB ou memória)\n",
    "if not dados_para_analise.empty:\n",
    "    dados_processamento = dados_para_analise.copy()\n",
    "\n",
    "    # 1. Análise Exploratória (Visualização)\n",
    "    print(\"\\n--- Exploração de Dados ---\")\n",
    "    # Garante que volume seja numérico, tratando erros\n",
    "    dados_processamento['volume'] = pd.to_numeric(dados_processamento['volume'], errors='coerce')\n",
    "    print(dados_processamento.describe())\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(dados_processamento['data_pregao'], dados_processamento['fechamento'], label='Preço de Fechamento')\n",
    "    plt.title(f'Preço de Fechamento {ticker}')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Preço (USD)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Engenharia de Features\n",
    "    print(\"\\n--- Engenharia de Features ---\")\n",
    "    dados_processamento['media_movel'] = dados_processamento['fechamento'].rolling(window=7).mean()\n",
    "    dados_processamento['retorno_diario'] = dados_processamento['fechamento'].pct_change()\n",
    "    print(\"Features 'media_movel' e 'retorno_diario' criadas.\")\n",
    "\n",
    "    # 3. Limpeza (Remover NaNs)\n",
    "    print(\"\\n--- Limpeza de Dados ---\")\n",
    "    print(f\"Registros antes do dropna: {len(dados_processamento)}\")\n",
    "    dados_processamento.dropna(inplace=True)\n",
    "    print(f\"Registros após o dropna: {len(dados_processamento)}\")\n",
    "\n",
    "    if not dados_processamento.empty:\n",
    "        # 4. Normalização\n",
    "        print(\"\\n--- Normalização (MinMaxScaler) ---\")\n",
    "        scaler = MinMaxScaler()\n",
    "        features_para_escalar = ['fechamento', 'volume', 'media_movel', 'retorno_diario']\n",
    "        # Verifica se todas as colunas existem antes de escalar\n",
    "        cols_existentes = [col for col in features_para_escalar if col in dados_processamento.columns]\n",
    "        if len(cols_existentes) == len(features_para_escalar):\n",
    "            dados_processamento[features_para_escalar] = scaler.fit_transform(dados_processamento[features_para_escalar])\n",
    "            print(\"Features escaladas:\")\n",
    "            print(dados_processamento[features_para_escalar].head())\n",
    "\n",
    "            # Salva o scaler para uso posterior\n",
    "            scaler_filename = 'scaler.pkl'\n",
    "            try:\n",
    "                with open(scaler_filename, 'wb') as f:\n",
    "                    pickle.dump(scaler, f)\n",
    "                print(f\"Scaler salvo como '{scaler_filename}'\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Erro ao salvar scaler: {e}\")\n",
    "\n",
    "\n",
    "            # 5. Divisão Treino/Teste\n",
    "            print(\"\\n--- Divisão Treino/Teste ---\")\n",
    "            features = features_para_escalar # Usa as mesmas features escaladas\n",
    "            target = 'fechamento' # Queremos prever o fechamento\n",
    "\n",
    "            X = dados_processamento[features]\n",
    "            y = dados_processamento[target]\n",
    "\n",
    "            # Dividindo os dados (80% treino, 20% teste), sem embaralhar\n",
    "            X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "            print(f\"Tamanho do conjunto de Treino (X): {X_treino.shape}\")\n",
    "            print(f\"Tamanho do conjunto de Teste (X): {X_teste.shape}\")\n",
    "        else:\n",
    "            print(f\"ERRO: Nem todas as colunas {features_para_escalar} encontradas para escalar.\")\n",
    "    else:\n",
    "        print(\"ERRO: Não há dados após a limpeza para continuar o pré-processamento.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nERRO: Sem dados para processar. Verifique a coleta e a conexão com o banco de dados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7cd436",
   "metadata": {},
   "source": [
    "## 5. Treinamento do Modelo de Machine Learning\n",
    "\n",
    "Com os dados preparados, podemos treinar nosso modelo. Escolhemos o **Random Forest Regressor**, um modelo de ensemble robusto e eficaz para tarefas de regressão, como a previsão de preços. [cite: main.py, techchallenge3/README.md]\n",
    "\n",
    "Para encontrar os melhores hiperparâmetros (configurações) do modelo, utilizamos o `GridSearchCV`, que testa várias combinações e seleciona a melhor com base na validação cruzada. [cite: main.py]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89618a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se X_treino e y_treino existem e não estão vazios\n",
    "if 'X_treino' in locals() and 'y_treino' in locals() and not X_treino.empty:\n",
    "    print(\"\\n--- Treinamento do Modelo (Random Forest Regressor) ---\")\n",
    "\n",
    "    # Definição da grade de hiperparâmetros (pode usar a original do main.py)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300], # Conforme main.py\n",
    "        'max_depth': [5, 10, 15],         # Conforme main.py\n",
    "        'min_samples_split': [2, 5, 10],  # Conforme main.py\n",
    "        'min_samples_leaf': [1, 2, 4]     # Conforme main.py\n",
    "    }\n",
    "    # param_grid_reduzida = { # Use esta se quiser teste mais rápido\n",
    "    #    'n_estimators': [100, 150],\n",
    "    #    'max_depth': [5, 10],\n",
    "    #    'min_samples_split': [5, 10],\n",
    "    #    'min_samples_leaf': [2, 4]\n",
    "    # }\n",
    "\n",
    "\n",
    "    # Inicializa o modelo\n",
    "    modelo_rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Configura o GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=modelo_rf, param_grid=param_grid, cv=3,\n",
    "                               scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "    # Treina o modelo usando GridSearchCV\n",
    "    print(\"Iniciando GridSearchCV...\")\n",
    "    grid_search.fit(X_treino, y_treino)\n",
    "\n",
    "    # Obtém o melhor modelo\n",
    "    melhor_modelo = grid_search.best_estimator_\n",
    "\n",
    "    print(f\"\\nMelhores hiperparâmetros encontrados: {grid_search.best_params_}\")\n",
    "    print(\"Modelo treinado com sucesso!\")\n",
    "\n",
    "    # Salva o modelo treinado localmente\n",
    "    modelo_filename = 'modelo.pkl'\n",
    "    try:\n",
    "        with open(modelo_filename, 'wb') as f:\n",
    "            pickle.dump(melhor_modelo, f)\n",
    "        print(f\"Modelo treinado salvo como '{modelo_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar o modelo: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nERRO: Conjuntos de treino (X_treino, y_treino) não disponíveis ou vazios. Treinamento pulado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651d493",
   "metadata": {},
   "source": [
    "## 6. Avaliação do Modelo\n",
    "\n",
    "Após o treinamento, precisamos avaliar o desempenho do modelo no conjunto de teste (dados que ele nunca viu antes). Usamos as métricas:\n",
    "\n",
    "* **Mean Squared Error (MSE):** Média dos erros quadrados. Penaliza mais os erros grandes.\n",
    "* **Mean Absolute Error (MAE):** Média dos erros absolutos. Mais interpretável na unidade original do alvo (após reverter a escala). [cite: main.py]\n",
    "\n",
    "Também visualizamos as previsões do modelo em comparação com os valores reais no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a01a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se o modelo foi treinado e os dados de teste existem\n",
    "if 'melhor_modelo' in locals() and 'X_teste' in locals() and not X_teste.empty:\n",
    "    print(\"\\n--- Avaliação do Modelo ---\")\n",
    "\n",
    "    # Carrega o scaler salvo anteriormente\n",
    "    scaler_carregado = None\n",
    "    scaler_filename = 'scaler.pkl'\n",
    "    try:\n",
    "        with open(scaler_filename, 'rb') as f:\n",
    "            scaler_carregado = pickle.load(f)\n",
    "        print(f\"Scaler '{scaler_filename}' carregado para avaliação.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar scaler '{scaler_filename}': {e}. A avaliação desnormalizada pode falhar.\")\n",
    "\n",
    "\n",
    "    # Fazer previsões no conjunto de teste\n",
    "    previsoes_teste_scaled = melhor_modelo.predict(X_teste)\n",
    "\n",
    "    # Calcular métricas com dados escalados\n",
    "    mse_scaled = mean_squared_error(y_teste, previsoes_teste_scaled)\n",
    "    mae_scaled = mean_absolute_error(y_teste, previsoes_teste_scaled)\n",
    "    print(f\"\\nMSE (escalado): {mse_scaled:.6f}\")\n",
    "    print(f\"MAE (escalado): {mae_scaled:.6f}\")\n",
    "\n",
    "    # Reverter a escala para interpretação (se o scaler foi carregado)\n",
    "    if scaler_carregado:\n",
    "        try:\n",
    "            # Cria cópias para evitar SettingWithCopyWarning\n",
    "            X_teste_copy = X_teste.copy()\n",
    "            X_teste_copy['target_pred_scaled'] = previsoes_teste_scaled\n",
    "            X_teste_copy['target_real_scaled'] = y_teste\n",
    "\n",
    "            # Recria o array completo para inverse_transform\n",
    "            # Previsões - usa valores reais das outras features + previsão da target\n",
    "            temp_pred_array = X_teste.copy()\n",
    "            temp_pred_array['fechamento'] = previsoes_teste_scaled\n",
    "            previsoes_teste_unscaled = scaler_carregado.inverse_transform(temp_pred_array)[:, features.index(target)]\n",
    "\n",
    "\n",
    "            # Reais - usa valores reais das outras features + valor real da target\n",
    "            temp_real_array = X_teste.copy()\n",
    "            temp_real_array['fechamento'] = y_teste\n",
    "            y_teste_unscaled = scaler_carregado.inverse_transform(temp_real_array)[:, features.index(target)]\n",
    "\n",
    "\n",
    "            # Calcular métricas com dados na escala original\n",
    "            mse_unscaled = mean_squared_error(y_teste_unscaled, previsoes_teste_unscaled)\n",
    "            mae_unscaled = mean_absolute_error(y_teste_unscaled, previsoes_teste_unscaled)\n",
    "            print(f\"\\nMSE (escala original): {mse_unscaled:.4f}\")\n",
    "            print(f\"MAE (escala original): {mae_unscaled:.4f} (Erro médio em USD)\")\n",
    "\n",
    "            # Visualização das Previsões vs Valores Reais\n",
    "            plt.figure(figsize=(14, 5))\n",
    "            # Usa o índice original do dataframe pré-processado para o eixo X\n",
    "            index_teste = dados_processamento.index[len(X_treino):]\n",
    "            plt.plot(index_teste, y_teste_unscaled, label='Valores Reais (Teste)', color='blue', marker='.', linestyle='None', alpha=0.7)\n",
    "            plt.plot(index_teste, previsoes_teste_unscaled, label='Previsões do Modelo (Teste)', color='red', alpha=0.7)\n",
    "            plt.title('Previsões do Modelo vs Valores Reais (Conjunto de Teste)')\n",
    "            plt.xlabel('Índice Original dos Dados')\n",
    "            plt.ylabel('Preço de Fechamento (USD)')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro durante a desnormalização ou plotagem: {e}\")\n",
    "            print(\"Plotando apenas os dados escalados:\")\n",
    "            # Fallback para plotar dados escalados se a desnormalização falhar\n",
    "            plt.figure(figsize=(14, 5))\n",
    "            index_teste = dados_processamento.index[len(X_treino):]\n",
    "            plt.plot(index_teste, y_teste, label='Valores Reais Scaled (Teste)', color='blue', marker='.', linestyle='None', alpha=0.7)\n",
    "            plt.plot(index_teste, previsoes_teste_scaled, label='Previsões Scaled (Teste)', color='red', alpha=0.7)\n",
    "            plt.title('Previsões Scaled vs Reais Scaled (Conjunto de Teste)')\n",
    "            plt.xlabel('Índice Original dos Dados')\n",
    "            plt.ylabel('Valor Escalado (0-1)')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"Não foi possível carregar o scaler. Plot e métricas desnormalizadas foram pulados.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nERRO: Modelo não treinado ou dados de teste não disponíveis. Avaliação pulada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daad0b6",
   "metadata": {},
   "source": [
    "## 7. API Flask e Previsão\n",
    "\n",
    "Para tornar o modelo útil, criamos uma API web usando Flask. A API tem rotas para: [cite: main.py, 6, 11]\n",
    "\n",
    "* `/historico`: Retornar os dados históricos armazenados no banco. [cite: main.py]\n",
    "* `/treinarmodelo`: Disparar o processo de coleta, armazenamento e treinamento (como fizemos nas etapas anteriores). [cite: main.py]\n",
    "* `/prever`: Receber (ou coletar internamente) os dados mais recentes, pré-processá-los *usando o mesmo scaler* que foi salvo, fazer a previsão com o modelo treinado e retornar o valor previsto (revertido para a escala original). [cite: main.py]\n",
    "* `/`: Exibir o dashboard. [cite: main.py]\n",
    "\n",
    "Abaixo, simulamos a lógica da rota `/prever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Simulação da Lógica de Previsão (como na API /prever) ---\")\n",
    "\n",
    "# 1. Carregar o modelo e scaler salvos localmente\n",
    "modelo_carregado = None\n",
    "scaler_carregado = None\n",
    "modelo_filename = 'modelo.pkl'\n",
    "scaler_filename = 'scaler.pkl'\n",
    "\n",
    "try:\n",
    "    with open(modelo_filename, 'rb') as f:\n",
    "        modelo_carregado = pickle.load(f)\n",
    "    print(f\"Modelo '{modelo_filename}' carregado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar modelo '{modelo_filename}': {e}\")\n",
    "\n",
    "try:\n",
    "    with open(scaler_filename, 'rb') as f:\n",
    "        scaler_carregado = pickle.load(f)\n",
    "    print(f\"Scaler '{scaler_filename}' carregado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar scaler '{scaler_filename}': {e}\")\n",
    "\n",
    "\n",
    "# Procede apenas se ambos foram carregados e temos dados de teste\n",
    "if modelo_carregado and scaler_carregado and 'X_teste' in locals() and not X_teste.empty:\n",
    "\n",
    "    # 2. Obter os dados mais recentes (simulamos usando a última linha do conjunto de teste)\n",
    "    # Na API real, buscaríamos com coletar_dados_tempo_real() e aplicaríamos\n",
    "    # o pré-processamento (rolling, pct_change, dropna, scale)\n",
    "    dados_recentes_features_scaled = X_teste.iloc[-1:].copy() # Pega a última linha como um DataFrame\n",
    "    print(\"\\nDados de entrada (simulados da última linha do teste, já escalados):\")\n",
    "    print(dados_recentes_features_scaled)\n",
    "\n",
    "    # 3. Fazer a previsão com o modelo carregado\n",
    "    previsao_scaled = modelo_carregado.predict(dados_recentes_features_scaled)[0]\n",
    "    print(f\"\\nPrevisão (escalada): {previsao_scaled:.6f}\")\n",
    "\n",
    "    # 4. Reverter a previsão para a escala original\n",
    "    try:\n",
    "        # Cria um array com a mesma estrutura das features originais,\n",
    "        # substituindo o valor da feature 'fechamento' pela previsão escalada.\n",
    "        input_data_for_inverse = dados_recentes_features_scaled.values[0].copy() # Faz uma cópia para não alterar o original\n",
    "        target_index = features.index('fechamento') # 'features' foi definido na célula de pré-proc\n",
    "        input_data_for_inverse[target_index] = previsao_scaled\n",
    "\n",
    "        # Aplica inverse_transform no array completo\n",
    "        previsao_unscaled = scaler_carregado.inverse_transform([input_data_for_inverse])[0, target_index]\n",
    "\n",
    "        print(f\"\\nPrevisão Final (escala original - USD): ${previsao_unscaled:.2f}\")\n",
    "\n",
    "        # Guarda a previsão para usar no gráfico final\n",
    "        previsao_final_simulada = previsao_unscaled\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao reverter a escala da previsão: {e}\")\n",
    "        previsao_final_simulada = None\n",
    "\n",
    "else:\n",
    "    print(\"\\nNão foi possível carregar o modelo e/ou scaler ou não há dados de teste. Simulação da previsão pulada.\")\n",
    "    previsao_final_simulada = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419022b7",
   "metadata": {},
   "source": [
    "## 8. Dashboard\n",
    "\n",
    "Finalmente, para apresentar os resultados de forma visual e acessível, criamos um dashboard simples usando Flask e HTML. [cite: 11, main.py, templates/dashboard.html]\n",
    "\n",
    "O dashboard exibe:\n",
    "1.  **Gráfico Histórico:** Um gráfico gerado com Matplotlib mostrando a evolução do preço de fechamento, lido do banco de dados. [cite: main.py]\n",
    "2.  **Previsão para o Próximo Dia:** O valor previsto pelo modelo através da rota `/prever`. [cite: main.py]\n",
    "\n",
    "A API Flask gera o gráfico, o converte para base64 e o embute diretamente na página HTML (`dashboard.html`) junto com o valor da previsão. [cite: main.py, templates/dashboard.html]\n",
    "\n",
    "Abaixo, mostramos como o gráfico histórico é gerado (sem a parte do Flask/HTML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc61d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Simulação da Geração do Gráfico para o Dashboard ---\")\n",
    "\n",
    "# Usa os dados lidos do banco ou da memória (já em dados_para_analise)\n",
    "if 'dados_para_analise' in locals() and not dados_para_analise.empty:\n",
    "    dados_grafico = dados_para_analise.copy()\n",
    "    # Garante que data_pregao seja datetime\n",
    "    dados_grafico['data_pregao'] = pd.to_datetime(dados_grafico['data_pregao'])\n",
    "\n",
    "\n",
    "    # Pega a previsão final simulada na célula anterior\n",
    "    previsao_amanha_simulada = previsao_final_simulada if 'previsao_final_simulada' in locals() else None\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dados_grafico['data_pregao'], dados_grafico['fechamento'], label='Preço de Fechamento Histórico', color='blue')\n",
    "\n",
    "    # Adiciona a previsão simulada ao gráfico (se disponível e válida)\n",
    "    if previsao_amanha_simulada is not None and pd.notna(previsao_amanha_simulada):\n",
    "        # Data de amanhã (baseada na última data dos dados)\n",
    "        ultima_data = dados_grafico['data_pregao'].iloc[-1]\n",
    "        data_previsao = ultima_data + timedelta(days=1)\n",
    "        plt.scatter(data_previsao, previsao_amanha_simulada, color='red', label=f'Previsão Próximo Dia (${previsao_amanha_simulada:.2f})', zorder=5, s=100)\n",
    "        print(f\"\\nAdicionando ponto de previsão para {data_previsao.strftime('%Y-%m-%d')} no valor de ${previsao_amanha_simulada:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nNenhuma previsão final válida para adicionar ao gráfico.\")\n",
    "\n",
    "    plt.title(f'Histórico de Preços de Fechamento {ticker} com Previsão Simulada')\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Preço (USD)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"\\nERRO: Não há dados carregados para gerar o gráfico final.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660cbd9",
   "metadata": {},
   "source": [
    "## 9. Conclusão\n",
    "\n",
    "Este projeto demonstrou o fluxo completo de um projeto de Machine Learning aplicado à previsão de preços de ações, cumprindo os requisitos do Tech Challenge: [cite: 6, 7, 8, 11]\n",
    "\n",
    "1.  **Coleta e Armazenamento:** Dados da GOOG foram coletados via `yfinance` e estruturados para armazenamento em PostgreSQL com SQLAlchemy. [cite: main.py]\n",
    "2.  **Pré-processamento e Modelagem:** Realizamos engenharia de features, normalização e treinamos um modelo Random Forest Regressor com otimização de hiperparâmetros (`GridSearchCV`). [cite: main.py]\n",
    "3.  **Avaliação:** O modelo foi avaliado com métricas como MSE e MAE. [cite: main.py]\n",
    "4.  **Produção:** Uma API Flask foi criada para servir previsões e um dashboard simples para visualização dos resultados. [cite: main.py, 11]\n",
    "5.  **Documentação:** O código está documentado e disponível no GitHub. [cite: 8, techchallenge3/README.md]\n",
    "\n",
    "**Próximos Passos Possíveis:**\n",
    "* Experimentar outros modelos (LSTM, ARIMA).\n",
    "* Adicionar mais features (sentimento de notícias, indicadores macroeconômicos).\n",
    "* Melhorar o dashboard com mais interatividade.\n",
    "* Deploy da solução em nuvem (AWS, GCP, Azure).\n",
    "\n",
    "Obrigado!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
